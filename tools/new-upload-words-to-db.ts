#!/usr/bin/env node --experimental-strip-types

import { Client } from 'pg';
import { createReadStream, readFileSync } from 'fs';
import { createInterface } from 'readline';
import { join, dirname } from 'path';
import { fileURLToPath } from 'url';

const TOOLS_DIR = dirname(fileURLToPath(import.meta.url));
const ROOT_DIR = join(TOOLS_DIR, '..');
const WORDS_FINAL_DIR = join(ROOT_DIR, 'words-final');
const WORD_LIST_FILE = join(WORDS_FINAL_DIR, 'word-list.csv');
const HINT_LIST_FILE = join(WORDS_FINAL_DIR, 'hints.csv');
const EMBEDDINGS_FILE = join(WORDS_FINAL_DIR, 'gemini_3072_embeddings.csv');

const VECTOR_DIMENSION = 3072;
const BATCH_SIZE = 500; // also our logging cadence

function unquoteCsvField(s: string): string {
  const t = s.trim();
  if (t.startsWith('"') && t.endsWith('"')) {
    return t.slice(1, -1).replace(/""/g, '"');
  }
  return t;
}

function parseEmbeddingCsvLine(line: string): { word: string; values: number[] } | null {
  const trimmed = line.trim();
  if (!trimmed) return null;

  const firstComma = trimmed.indexOf(',');
  if (firstComma === -1) return null;
  const word = unquoteCsvField(trimmed.slice(0, firstComma));
  const rest = trimmed.slice(firstComma + 1).trim();
  if (!word) return null;

  // Case 1: second column is a JSON array (possibly quoted as CSV field)
  let arrayText = rest;
  if (arrayText.startsWith('"') && arrayText.endsWith('"')) {
    arrayText = unquoteCsvField(arrayText);
  }
  if (arrayText.startsWith('[') && arrayText.endsWith(']')) {
    try {
      const parsed = JSON.parse(arrayText);
      if (Array.isArray(parsed) && parsed.length > 0) {
        const asNums = parsed.map((v: unknown) => Number(v));
        if (asNums.every((n) => Number.isFinite(n))) {
          return { word, values: asNums };
        }
      }
    } catch {
      // fall through
    }
  }

  // Case 2: simple CSV across many numeric columns
  const parts = rest.split(',');
  if (parts.length < 2) return null;
  const nums = parts.map((p) => Number(p));
  if (!nums.every((n) => Number.isFinite(n))) return null;
  return { word, values: nums };
}

function loadWordSetFromCsv(filePath: string): Set<string> {
  const set = new Set<string>();
  const content = readFileSync(filePath, 'utf8');
  let seenHeader = false;
  for (const rawLine of content.split(/\r?\n/)) {
    const line = rawLine.trim();
    if (!line) continue;
    if (!seenHeader) {
      seenHeader = true; // skip the first non-empty line as header
      continue;
    }
    const idx = line.indexOf(',');
    const field = idx === -1 ? line : line.slice(0, idx);
    const word = unquoteCsvField(field);
    if (word) set.add(word);
  }
  return set;
}

function formatVectorLiteral(values: number[]): string {
  // pgvector accepts '[v1, v2, ...]' literal
  // Ensure finite numbers and preserve precision
  return `[${values.map((n) => (Number.isFinite(n) ? n : 0)).join(', ')}]`;
}

async function ensureDatabase(client: Client) {
  await client.query('CREATE EXTENSION IF NOT EXISTS vector');
  await client.query(
    `CREATE TABLE IF NOT EXISTS words_2 (
      id integer generated by default as identity primary key,
      word varchar(255) not null unique,
      embedding_3072 vector(${VECTOR_DIMENSION}),
      is_hint boolean not null default false
    )`
  );
}

type RowToInsert = { word: string; vectorLiteral: string; isHint: boolean };

async function insertBatch(client: Client, batch: RowToInsert[]) {
  if (batch.length === 0) return;
  const valuesSql: string[] = [];
  const params: unknown[] = [];
  let paramIndex = 1;
  for (const row of batch) {
    valuesSql.push(`($${paramIndex++}, $${paramIndex++}::vector, $${paramIndex++})`);
    params.push(row.word, row.vectorLiteral, row.isHint);
  }
  const sql = `
    INSERT INTO words_2 (word, embedding_3072, is_hint)
    VALUES ${valuesSql.join(', ')}
    ON CONFLICT (word) DO UPDATE
      SET embedding_3072 = EXCLUDED.embedding_3072,
          is_hint = EXCLUDED.is_hint
  `;
  await client.query('BEGIN');
  try {
    await client.query(sql, params);
    await client.query('COMMIT');
  } catch (err) {
    await client.query('ROLLBACK');
    throw err;
  }
}

async function main() {
  const password = process.env.SUPABASE_DB_PASSWORD;
  if (!password) {
    console.error(
      'Missing SUPABASE_DB_PASSWORD environment variable (expected to contain the database password).'
    );
    process.exit(1);
  }

  const client = new Client({
    host: 'aws-0-us-east-1.pooler.supabase.com',
    port: 5432,
    user: 'postgres.jbbhyxtpholdwrxencjx',
    password,
    database: 'postgres',
    ssl: { rejectUnauthorized: false },
    keepAlive: true,
    connectionTimeoutMillis: 15000,
  });
  await client.connect();
  try {
    await ensureDatabase(client);
    // Clear existing rows before re-importing
    await client.query('TRUNCATE TABLE words_2 RESTART IDENTITY');

    const masterWords = loadWordSetFromCsv(WORD_LIST_FILE);
    const hintWords = loadWordSetFromCsv(HINT_LIST_FILE);

    const rl = createInterface({
      input: createReadStream(EMBEDDINGS_FILE, { encoding: 'utf8' }),
      crlfDelay: Infinity,
    });

    let processed = 0;
    let inserted = 0;
    let dim: number | null = null;
    let batch: RowToInsert[] = [];
    let skippedEmbeddingHeader = false;

    for await (const line of rl) {
      const trimmed = line.trim();
      if (!trimmed) continue;
      if (!skippedEmbeddingHeader) {
        skippedEmbeddingHeader = true; // skip first non-empty line as header
        continue;
      }
      const parsed = parseEmbeddingCsvLine(line);
      if (!parsed) continue;
      processed++;
      if (dim == null) dim = parsed.values.length;
      if (parsed.values.length !== VECTOR_DIMENSION) {
        // Skip rows with unexpected dimension to avoid server-side errors
        continue;
      }
      if (!masterWords.has(parsed.word)) continue;

      const vectorLiteral = formatVectorLiteral(parsed.values);
      const isHint = hintWords.has(parsed.word);
      batch.push({ word: parsed.word, vectorLiteral, isHint });

      if (batch.length >= BATCH_SIZE) {
        await insertBatch(client, batch);
        inserted += batch.length;
        console.log(`... processed ${inserted} rows`);
        batch = [];
      }
    }

    if (batch.length > 0) {
      await insertBatch(client, batch);
      inserted += batch.length;
      console.log(`... processed ${inserted} rows`);
      batch = [];
    }

    console.log(`âœ… Uploaded ${inserted} words to words_2 (dim=${dim ?? 0})`);
  } finally {
    await client.end();
  }
}

main().catch((err) => {
  console.error(err);
  process.exit(1);
});
